[
  {
    "objectID": "pages/ordinal-regression/index.html",
    "href": "pages/ordinal-regression/index.html",
    "title": "Ordinal Regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "pages/ordinal-regression/index.html#instructions",
    "href": "pages/ordinal-regression/index.html#instructions",
    "title": "Ordinal Regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "pages/ordinal-regression/index.html#load-packages",
    "href": "pages/ordinal-regression/index.html#load-packages",
    "title": "Ordinal Regression",
    "section": "Load packages:",
    "text": "Load packages:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(performance)\nlibrary(ordinal) #clm\nlibrary(car) # anova\nlibrary(ggeffects) #  viz\nlibrary(gofcat) # brant\nlibrary(brms)\nlibrary(emmeans) # contrasts\nlibrary(knitr)"
  },
  {
    "objectID": "pages/ordinal-regression/index.html#load-data",
    "href": "pages/ordinal-regression/index.html#load-data",
    "title": "Ordinal Regression",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\n\nCode\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Filter to keep only the top 3 ranks\ngb &lt;- gbbo %&gt;% filter(`Technical Rank` %in% c(1, 2, 3))"
  },
  {
    "objectID": "pages/ordinal-regression/index.html#explore",
    "href": "pages/ordinal-regression/index.html#explore",
    "title": "Ordinal Regression",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank— create one for Gender and Age\n\n\n\nCode\n# Plot percentage of bakers in each rank by Gender\ngb %&gt;%\n  group_by(Gender, `Technical Rank`) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = count / sum(count) * 100) %&gt;%\n  ggplot(aes(x = `Technical Rank`, y = percentage, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Percentage of Bakers in Each Rank by Gender\", x = \"Technical Rank\", y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n # Plot percentage of bakers in each rank by Age\ngbbo %&gt;%\n  group_by(Age, `Technical Rank`) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = count / sum(count) * 100) %&gt;%\n  ggplot(aes(x = `Technical Rank`, y = percentage, fill = as.factor(Age))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Percentage of Bakers in Each Rank by Age\", x = \"Technical Rank\", y = \"Percentage\") +\n  theme_minimal()"
  },
  {
    "objectID": "pages/ordinal-regression/index.html#ordinal-analysis",
    "href": "pages/ordinal-regression/index.html#ordinal-analysis",
    "title": "Ordinal Regression",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven’t already, convert the outcome variable to an ordered factor. What does the order here represent?\n\n\n\nCode\n# Convert the outcome variable to an ordered factor\ngb$Technical_Rank &lt;- factor(gb$`Technical Rank`, levels = c(1, 2, 3), ordered = TRUE)\n# The order here represents the rank in technical, with 1 being the highest rank and 3 being the lowest among the top 3.\n\n\n\nConvert input variables to categorical factors as appropriate.\n\n\n\nCode\n# Convert input variables to categorical factors as appropriate\ngb$Gender &lt;- as.factor(gb$Gender)\n\n\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\n\nCode\n# Run an ordinal logistic regression model\nmodel &lt;- clm(Technical_Rank ~ Gender * Age, data = gb)\n\n# Display the summary of the model\nsummary(model)\n\n\nformula: Technical_Rank ~ Gender * Age\ndata:    gb\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  309  -336.64 683.28 3(0)  4.04e-08 1.1e+05\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \nGenderM     -1.14947    0.67290  -1.708   0.0876 .\nAge         -0.02311    0.01246  -1.855   0.0636 .\nGenderM:Age  0.03879    0.01853   2.093   0.0363 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n1|2 -1.416692   0.459602  -3.082\n2|3  0.004944   0.452043   0.011"
  },
  {
    "objectID": "pages/poisson-regression/index.html",
    "href": "pages/poisson-regression/index.html",
    "title": "Poission Regression",
    "section": "",
    "text": "To complete this lab:\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(easystats)\nlibrary(performance)\nlibrary(knitr)\nCode\nlibrary(tidyverse)\n\ndata &lt;- read_delim(\"https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/2010.csv\")\nCode\nlibrary(naniar)\n\ndata_pos &lt;- data %&gt;%\n  dplyr::select(wwwhr, wordsum, age, sex, reliten, polviews, wrkhome) %&gt;%\nreplace_with_na(.,\n             replace = list(wwwhr = c(-1, 998, 999),\n                          wordsum = c(-1, 99),\n                          reliten = c(0, 8, 9), \n             polviews = c(0, 8, 9), \n             wrkhome = c(0,8,9), \n             age=c(0, 98, 99)))\nQ: Can you explain what might be going on in the above code?\nA: Focuses on a subset of columns from the original dataset. Replaces specific “invalid” or “placeholder” values (e.g., -1, 998, 999) with NA to mark them as missing data.\nQ: The next step in data cleaning would be to ensure that the data in your code are aligned with the description/ usage context of the variables\nCode\ndata_pos &lt;- data_pos %&gt;%\n  mutate(\n    sex = factor(sex, levels = c(1, -1), labels = c(\"Male\", \"Female\"))\n  )\n  \n  # Recode 'reliten' into a factor with meaningful labels\n data_pos &lt;- data_pos %&gt;%\n  mutate(\n    reliten_recode = factor(reliten, levels = c(1, 2, 3, 4),\n                     labels = c(\"Extremely important\", \"Very important\", \n                                \"Somewhat important\", \"Not very important\"),\n                     ordered = TRUE)\n  )"
  },
  {
    "objectID": "pages/poisson-regression/index.html#missingness",
    "href": "pages/poisson-regression/index.html#missingness",
    "title": "Poission Regression",
    "section": "Missingness",
    "text": "Missingness\n\n\nCode\ndata_pos %&gt;%\n  dplyr::select(reliten, reliten_recode)\n\n\n# A tibble: 2,044 × 2\n   reliten reliten_recode     \n     &lt;dbl&gt; &lt;ord&gt;              \n 1       1 Extremely important\n 2       4 Not very important \n 3       1 Extremely important\n 4       1 Extremely important\n 5       1 Extremely important\n 6       4 Not very important \n 7       3 Somewhat important \n 8       1 Extremely important\n 9       1 Extremely important\n10       1 Extremely important\n# ℹ 2,034 more rows\n\n\nCode\nlibrary(skimr)\nskimr::skim(data_pos)\n\n\n\nData summary\n\n\nName\ndata_pos\n\n\nNumber of rows\n2044\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1.00\nFALSE\n2\nFem: 1153, Mal: 891\n\n\nreliten_recode\n99\n0.95\nTRUE\n4\nVer: 747, Ext: 707, Not: 363, Som: 128\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwwwhr\n996\n0.51\n9.79\n13.41\n0\n2\n5\n14\n168\n▇▁▁▁▁\n\n\nwordsum\n657\n0.68\n6.03\n2.07\n0\n5\n6\n7\n10\n▁▃▇▅▂\n\n\nage\n3\n1.00\n47.97\n17.68\n18\n33\n47\n61\n89\n▇▇▇▅▃\n\n\nreliten\n99\n0.95\n2.08\n1.08\n1\n1\n2\n3\n4\n▇▇▁▂▃\n\n\npolviews\n71\n0.97\n4.08\n1.46\n1\n3\n4\n5\n7\n▃▂▇▃▅\n\n\nwrkhome\n882\n0.57\n2.26\n1.72\n1\n1\n1\n4\n6\n▇▁▁▂▁\n\n\n\n\n\n\n\nCode\ntable(data_pos$sex, useNA = \"ifany\")\n\n\n\n  Male Female \n   891   1153"
  },
  {
    "objectID": "pages/poisson-regression/index.html#fit-a-poisson-model-to-the-data.",
    "href": "pages/poisson-regression/index.html#fit-a-poisson-model-to-the-data.",
    "title": "Poission Regression",
    "section": "Fit a Poisson model to the data.",
    "text": "Fit a Poisson model to the data.\n\n\nCode\npoisson_model &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome, \n                     data = data_pos, \n                     family = poisson)"
  },
  {
    "objectID": "pages/poisson-regression/index.html#carry-out-model-checking",
    "href": "pages/poisson-regression/index.html#carry-out-model-checking",
    "title": "Poission Regression",
    "section": "Carry out model checking",
    "text": "Carry out model checking\nHint: performance package has the function you’re looking for\n\n\nCode\n# Model checking using the performance package\nlibrary(performance)\n\ncheck_model(poisson_model)"
  },
  {
    "objectID": "pages/poisson-regression/index.html#find-any-outliers",
    "href": "pages/poisson-regression/index.html#find-any-outliers",
    "title": "Poission Regression",
    "section": "Find any outliers",
    "text": "Find any outliers\n\n\nCode\n# Find outliers\noutliers &lt;- check_outliers(poisson_model)\n\n# View what `outliers` contains\nstr(outliers)\n\n\n 'check_outliers' logi [1:603] FALSE FALSE FALSE FALSE FALSE FALSE ...\n - attr(*, \"data\")='data.frame':    603 obs. of  4 variables:\n  ..$ Row          : int [1:603] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ Distance_Cook: num [1:603] 0.00844 0.00432 0.00755 0.00247 0.00284 ...\n  ..$ Outlier_Cook : num [1:603] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ Outlier      : num [1:603] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"threshold\")=List of 1\n  ..$ cook: num 0.892\n - attr(*, \"method\")= chr \"cook\"\n - attr(*, \"text_size\")= num 3\n - attr(*, \"influential_obs\")='data.frame': 603 obs. of  7 variables:\n  ..$ Hat           : num [1:603] 0.01134 0.03224 0.00785 0.00687 0.00503 ...\n  ..$ Cooks_Distance: num [1:603] 0.00844 0.00432 0.00755 0.00247 0.00284 ...\n  ..$ Predicted     : num [1:603] 10.21 24.65 8.51 9.97 7.38 ...\n  ..$ Residuals     : num [1:603] -2.659 -0.969 -3.277 -1.743 -2.352 ...\n  ..$ Std_Residuals : num [1:603] -2.674 -0.985 -3.29 -1.749 -2.358 ...\n  ..$ Index         : int [1:603] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ Influential   : chr [1:603] \"OK\" \"OK\" \"OK\" \"OK\" ...\n  ..- attr(*, \"cook_levels\")= Named num 0.892\n  .. ..- attr(*, \"names\")= chr \"cook\"\n  ..- attr(*, \"n_params\")= int 7\n - attr(*, \"variables\")= chr \"(Whole model)\"\n - attr(*, \"raw_data\")= tibble [603 × 6] (S3: tbl_df/tbl/data.frame)\n  ..$ wwwhr   : num [1:603] 3 20 1 5 2 15 20 14 3 4 ...\n  ..$ wordsum : num [1:603] 6 9 6 6 6 8 10 8 9 7 ...\n  ..$ age     : num [1:603] 31 23 31 35 49 35 21 33 53 57 ...\n  ..$ reliten : num [1:603] 1 4 2 2 2 4 4 2 2 1 ...\n  ..$ polviews: num [1:603] 3 2 2 3 5 2 3 4 3 5 ...\n  ..$ wrkhome : num [1:603] 3 4 1 1 1 2 1 1 4 4 ...\n - attr(*, \"outlier_var\")= list()\n - attr(*, \"outlier_count\")=List of 2\n  ..$ cook:'data.frame':    3 obs. of  2 variables:\n  .. ..$ Row   : int [1:3] 72 156 363\n  .. ..$ n_Cook: chr [1:3] \"(Multivariate)\" \"(Multivariate)\" \"(Multivariate)\"\n  ..$ all :'data.frame':    3 obs. of  2 variables:\n  .. ..$ Row   : num [1:3] 72 156 363\n  .. ..$ n_Cook: chr [1:3] \"(Multivariate)\" \"(Multivariate)\" \"(Multivariate)\""
  },
  {
    "objectID": "pages/poisson-regression/index.html#refit-the-model-after-excludint-outliers",
    "href": "pages/poisson-regression/index.html#refit-the-model-after-excludint-outliers",
    "title": "Poission Regression",
    "section": "Refit the model after excludint outliers",
    "text": "Refit the model after excludint outliers\n\n\nCode\noutlier_ids &lt;- attr(outliers, \"outlier_count\")$all$Row\n\n# Remove those rows from the original dataset\ndata_pos_clean &lt;- data_pos[-outlier_ids, ]\n\n# Refit the Poisson model without the outliers\npoisson_model_refit &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome, \n                           data = data_pos_clean, \n                           family = poisson)\n\n# Summarize the new model\nsummary(poisson_model_refit)\n\n\n\nCall:\nglm(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, family = poisson, data = data_pos_clean)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.906723   0.082546  23.099  &lt; 2e-16 ***\nwordsum      0.100003   0.007761  12.885  &lt; 2e-16 ***\nage         -0.016446   0.001088 -15.121  &lt; 2e-16 ***\nsexFemale   -0.261019   0.026403  -9.886  &lt; 2e-16 ***\nreliten      0.198362   0.011896  16.675  &lt; 2e-16 ***\npolviews    -0.035636   0.009684  -3.680 0.000233 ***\nwrkhome      0.079194   0.007677  10.316  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 7663.2  on 600  degrees of freedom\nResidual deviance: 6479.8  on 594  degrees of freedom\n  (1440 observations deleted due to missingness)\nAIC: 8530.5\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCode\nmodel_parameters(poisson_model_refit) %&gt;%\n  print_html()\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n1.91\n0.08\n(1.74, 2.07)\n23.10\n&lt; .001\n\n\nwordsum\n0.10\n7.76e-03\n(0.08, 0.12)\n12.88\n&lt; .001\n\n\nage\n-0.02\n1.09e-03\n(-0.02, -0.01)\n-15.12\n&lt; .001\n\n\nsex (Female)\n-0.26\n0.03\n(-0.31, -0.21)\n-9.89\n&lt; .001\n\n\nreliten\n0.20\n0.01\n(0.18, 0.22)\n16.67\n&lt; .001\n\n\npolviews\n-0.04\n9.68e-03\n(-0.05, -0.02)\n-3.68\n&lt; .001\n\n\nwrkhome\n0.08\n7.68e-03\n(0.06, 0.09)\n10.32\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck for Overdispersion\nHint: performance package has the function you’re looking for\n\n\nCode\ncheck_overdispersion(poisson_model_refit)\n\n\n# Overdispersion test\n\n       dispersion ratio =   14.671\n  Pearson's Chi-Squared = 8714.618\n                p-value =  &lt; 0.001\n\n\nWhat do you notice? And what’s a good next step forward? Can there be another model class that can fit the data? If so, fit this model to the data.\nThe dispersion ratio is very high (14.687), indicating strong overdispersion. This violates the assumptions of the Poisson model. A better model for overdispersed count data is the Negative Binomial model, which accounts for extra variance. I refit the model using glm.nb() from the MASS package.\n\n\nCode\n# Load necessary package\nlibrary(MASS)\n\n# Fit a Negative Binomial model to account for overdispersion\nnb_model &lt;- glm.nb(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome, \n                   data = data_pos_clean)\n\n# Display the summary of the model\nsummary(nb_model)\n\n\n\nCall:\nglm.nb(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, data = data_pos_clean, init.theta = 0.9608344909, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.817201   0.276005   6.584 4.58e-11 ***\nwordsum      0.109041   0.025967   4.199 2.68e-05 ***\nage         -0.015821   0.003533  -4.479 7.51e-06 ***\nsexFemale   -0.170200   0.089437  -1.903   0.0570 .  \nreliten      0.202860   0.041523   4.885 1.03e-06 ***\npolviews    -0.034491   0.033143  -1.041   0.2980    \nwrkhome      0.055261   0.027134   2.037   0.0417 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.9608) family taken to be 1)\n\n    Null deviance: 767.46  on 600  degrees of freedom\nResidual deviance: 669.38  on 594  degrees of freedom\n  (1440 observations deleted due to missingness)\nAIC: 3928.9\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.9608 \n          Std. Err.:  0.0593 \n\n 2 x log-likelihood:  -3912.9410"
  },
  {
    "objectID": "pages/poisson-regression/index.html#which-one-is-better--your-earlier-model-or-later-model",
    "href": "pages/poisson-regression/index.html#which-one-is-better--your-earlier-model-or-later-model",
    "title": "Poission Regression",
    "section": "Which one is better- your earlier model, or later model?",
    "text": "Which one is better- your earlier model, or later model?\nlibrary(performance)\n\n# Compare Poisson (refit) and Negative Binomial models\ncompare_performance(poisson_model_refit, nb_model)\nThe Negative Binomial model is clearly better based on model comparison: - Much lower AIC (3931.5 vs. 8515.5) - Higher log score and spherical score - Correctly accounts for overdispersion"
  },
  {
    "objectID": "pages/poisson-regression/index.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "href": "pages/poisson-regression/index.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "title": "Poission Regression",
    "section": "What is zero inflation? Is there zero-inflation in your chosen model?",
    "text": "What is zero inflation? Is there zero-inflation in your chosen model?\nZero inflation occurs when a dataset has more zeros than expected under a standard count model like Poisson or Negative Binomial. These “extra” zeros may come from a separate process (e.g., people who never use the internet at all, regardless of predictors).\n\n\nCode\ncheck_zeroinflation(nb_model)\n\n\n# Check for zero-inflation\n\n   Observed zeros: 40\n  Predicted zeros: 66\n            Ratio: 1.66\n\n\nThe model predicts more zeros (67) than actually observed (40), with a ratio of 1.68 — indicating it’s overfitting zeros.\n\nLog LambdaMean Count\n\n\n\n\nCode\n# Extract log(λ), which is the linear predictor\nlog_lambda &lt;- predict(nb_model, type = \"link\")\n\n# Optional: View first few values\nhead(log_lambda)\n\n\n       1        2       11       20       21       25 \n2.246158 3.227982 2.202787 2.275211 1.984732 2.818564 \n\n\nCode\n# Optional: Plot the distribution\nhist(log_lambda, main = \"Distribution of log(λ)\", xlab = \"log(λ)\", col = \"lightblue\", breaks = 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get the predicted mean count (λ)\nmean_count &lt;- predict(nb_model, type = \"response\")\n\n# Optional: View first few predictions\nhead(mean_count)\n\n\n        1         2        11        20        21        25 \n 9.451357 25.228696  9.050201  9.729971  7.277096 16.752777 \n\n\nCode\n# Optional: Plot the distribution\nhist(mean_count, main = \"Distribution of Predicted Counts (λ)\", \n     xlab = \"Predicted Count\", col = \"lightgreen\", breaks = 30)"
  },
  {
    "objectID": "pages/poisson-regression/index.html#report-your-conclusions",
    "href": "pages/poisson-regression/index.html#report-your-conclusions",
    "title": "Poission Regression",
    "section": "Report your conclusions",
    "text": "Report your conclusions\nWe began by fitting a Poisson regression model to predict weekly internet usage (wwwhr). Model diagnostics revealed significant overdispersion (dispersion ratio = 14.687, p &lt; .001), indicating that the Poisson model underestimated variance.\nTo address this, we refit the data using a Negative Binomial model, which appropriately handles overdispersion. Model comparison showed that the Negative Binomial model had a much lower AIC and better overall fit than the Poisson model.\nWe also tested for zero inflation. The model was found to overpredict zeros (67 predicted vs. 40 observed), so zero inflation is not a concern. Therefore, a zero-inflated model is not necessary.\nIn conclusion, the Negative Binomial model is the best-fitting and most appropriate model for this dataset. It provides a more reliable basis for interpreting predictors of internet use."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "504",
    "section": "",
    "text": "This is a Quarto website for the 504 course in the Department of Psychology at Princeton.\nThe course is taught by Professor Uri Hasson."
  },
  {
    "objectID": "pages/logistic-regression/index.html",
    "href": "pages/logistic-regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\nThe data are in gss2016.csv in the data folder.\n\n\n\n\nLet’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then plot the proportion of the response variable, using informative labels for each category.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(modelsummary)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(easystats)\nlibrary(broom)\nlibrary(emmeans)\nlibrary(marginaleffects)\nlibrary(performance)\nlibrary(arm)\nlibrary(modelsummary)\nlibrary(here)\n\n\n\n\nCode\n# Set the working directory\nsetwd(here::here(\"pages/logistic-regression\"))\n# load data\ndata &lt;- read.csv(\"gss2016.csv\")\n# look at the data \nhead(data)\n\n\n      natmass age    sex sei10      region         polviews\n1  Too little  47   Male  87.9 New england         Moderate\n2  Too little  61   Male  38.3 New england          Liberal\n3    Too much  43 Female  21.8 New england         Moderate\n4  Too little  55 Female  39.7 New england Slightly liberal\n5 About right  53 Female  44.6 New england Slightly liberal\n6  Too little  50   Male  80.7 New england Slightly liberal\n\n\nCode\n# See what `natmass` levels exist\nunique(data$natmass)\n\n\n[1] \"Too little\"  \"Too much\"    \"About right\"\n\n\nFill in the “____” below to encode the binary variable\n\n\nCode\ndata &lt;- data %&gt;%\n   mutate(mass_trans_spend_right = as.numeric(natmass == \"About right\"))\nhead(data$mass_trans_spend_right)\n\n\n[1] 0 0 0 0 1 0\n\n\n\n\nCode\n#Get proportions\nmass_spend_summary &lt;- data %&gt;%\n  count(mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if neederd\nmass_spend_long &lt;- mass_spend_summary %&gt;% mutate(opinion=\"Proportion\")\n\n#Factorise for plot\nmass_spend_long$mass_trans_spend_right &lt;- as.factor(mass_spend_long$mass_trans_spend_right)\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\nggplot(mass_spend_long, aes(x = opinion, y = proportion, fill = mass_trans_spend_right)) + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(polviews = factor(polviews,\n                           levels = c(\"Extremely liberal\", \"Liberal\", \"Slightly liberal\",\n                                      \"Moderate\", \"Slghtly conservative\",\"Conservative\", \"Extrmly conservative\"),\n                           ordered = TRUE))\n\n\n\nMake a plot of the distribution of polviews\n\n\n\nCode\n# Get proportions\npoly_summary &lt;- data %&gt;%\n  count(polviews) %&gt;%\n  mutate(proportion_o = n / sum(n))\n\n# Keep the data structure appropriate for plotting\npoly_long &lt;- poly_summary %&gt;% mutate(opinion=\"Proportion\")\n\n# Convert polviews into a factor if not already\npoly_long$polviews &lt;- as.factor(poly_long$polviews)\n\n# Plot the proportions\nggplot(poly_long, aes(x = opinion, y = proportion_o, fill = polviews)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\nWhich political view occurs most frequently in this data set?\nModerate\n\n\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\n\n\n\nCode\n# Plot the relationship between satisfaction with mass transportation spending and political views\nggplot(data, aes(x = polviews, fill = as.factor(mass_trans_spend_right))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Relationship between Political Views and Satisfaction with Mass Transportation Spending\",\n       x = \"Political Views\",\n       y = \"Proportion\",\n       fill = \"Satisfaction with Spending\") +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"green\"), labels = c(\"Not Satisfied\", \"Satisfied\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe more conservative one’s political views are the more they think the amount of spending on mass transportation is correct.\n\nWe’d like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as “89 or older”.\n\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values “89 or older” with a single value.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(age = if_else(age == \"89 or older\", \"89\", age), \n         age = as.numeric(age))\n\n\n\nPlot the frequency distribution of age.\n\n\n\nCode\nggplot(data, aes(x = age)) +\n  geom_histogram(binwidth = 1,color='black', alpha = 0.7) +\n  labs(title = \"Age Distribution\",\n       x = \"Age\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s start by fitting a logistic regression model with just the intercept\n\n\n\nCode\nintercept_only_model &lt;- glm(\n  mass_trans_spend_right ~ 1, \n  data = data, \n  family = binomial\n) \n\nintercept_only_model %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.1190594\n0.0393685\n3.024229\n0.0024927\n\n\n\n\n\n\nInterpret the intercept in the context of the data. You can do this by converting the \\(\\beta_0\\) parameter out of the log-odds metric to the probability metric. Make sure to include the 95% confidence intervals. Then interpret the results in a sentence or two–what is the basic thing this probability tells us about?\n\n\n\nCode\nb0 &lt;- coef(intercept_only_model)[1] # get coef\n\nb0_transformed &lt;- exp(b0) / (1 + exp(b0)) # logistic transform\n\nci_lower = b0 - 1.96 * 0.0393685\nci_upper = b0 + 1.96 * 0.0393685\n\n#transforming confidence intervals of coefficients into probabilities\np_lower = exp(ci_lower) / (1 + exp(ci_lower))\np_upper = exp(ci_upper) / (1 + exp(ci_upper))\n\nb0_transformed\n\n\n(Intercept) \n  0.5297297 \n\n\nCode\np_lower\n\n\n(Intercept) \n  0.5104727 \n\n\nCode\np_upper\n\n\n(Intercept) \n  0.5488986 \n\n\nInterpretation: The intercept-only model’s intercept (β0) represents the log-odds of a respondent being satisfied with mass transportation spending when all predictors are at their reference levels. Converting this to the probability metric, the probability of a respondent being satisfied with mass transportation spending is approximately 0.53 (95% CI [0.51, 0.55]). This tells us that, on average, about 53% of respondents are satisfied with the current spending on mass transportation.\n\nNow let’s fit a model using the demographic factors - age,sex, sei10 - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model coefficients (do not display the summary output)\n\n\n\nCode\n#make sure that sex is a factor (i.e. to make sure R knows it's binary/categorical, and not continuous)\ndata &lt;- data %&gt;%\n  mutate(sex = factor(sex, levels = c(\"Male\", \"Female\")))\n\n#fit with glm()\nm1 &lt;- glm(mass_trans_spend_right ~ age + sex + sei10, data = data, family = binomial)\n\n#produce tidy output of model coefficients\nm1 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.5697071\n0.1409061\n4.043169\n0.0000527\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n0.2557439\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\n\nConsider the relationship between sex and one’s opinion about spending on mass transportation. Interpret the coefficient of sex in terms of the logs odds and OR of being satisfied with spending on mass transportation. What are the predicted probabilities for males and females on support for spending on mass transportation? Please include the 95% CIs around each estimate.\n\n\n\nCode\nm1 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.5697071\n0.1409061\n4.043169\n0.0000527\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n0.2557439\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nm1 %&gt;% \n  tidy(exponentiate = TRUE) %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.7677492\n0.1409061\n4.043169\n0.0000527\n\n\nage\n0.9938530\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n1.2914219\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n0.9937922\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nbsex &lt;- coef(m1)[\"sexFemale\"]\n\nci_lower_lo = bsex - 1.96 * 0.0798020\nci_upper_lo = bsex + 1.96 * 0.0798020\n\nci_lower_or = 1.29 - 1.96 * 0.0798020\nci_upper_or = 1.29 + 1.96 * 0.0798020\n\nemm_sex &lt;- emmeans(m1, \"sex\", type = \"response\")\n\nci\n\n\nfunction (x, ...) \n{\n    UseMethod(\"ci\")\n}\n&lt;bytecode: 0x106aabe70&gt;\n&lt;environment: namespace:bayestestR&gt;\n\n\nIf you did this right, you’ll find that being female (as compared to male) is associated with an increase in the log-odds of being satisfied with spending on mass transportation by 0.2557439 units (95% CI [0.09, 0.41]), holding all other variables constant. This equates to the odds of thinking the spending amount is right in females being 1.29 times the odds of thinking this in men (95% CI [1.13, 1.44]).\nThe predicted probability for females to be satisfied with spending on mass transportation is 55.9% (95% CI [53.3%, 58.5%]) and that of males is 49.5% (95% CI [46.7%, 52.4%]).\n\nVerify this.\n\nNext, consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate.\n\n\n\nCode\nbage &lt;- coef(m1)[\"age\"]\n\nci_lower_age = bage - 1.96 * 0.005\nci_upper_age = bage + 1.96 * 0.005\n\n#transforming confidence intervals of coefficients into odds ratios\nor_age = exp(bage)\nor_lower_age = exp(ci_lower_age)\nor_upper_age = exp(ci_upper_age)\n\nbage\n\n\n        age \n-0.00616594 \n\n\nCode\nor_age\n\n\n     age \n0.993853 \n\n\nCode\nor_lower_age\n\n\n      age \n0.9841608 \n\n\nCode\nor_upper_age\n\n\n     age \n1.003641 \n\n\nA one unit increase in age is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by -0.0061659 (95% CI [-0.0159659, 0.0036341]), holding all other variables constant. The odds ratio is 0.993853 (95% CI [0.9841608, 1.0036407])which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of age, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993853, or approximately 0.6146969% per unit increase in age, holding other factors constant.\n\nConsider the relationship between SES and one’s opinion about spending on mass transportation. Interpret the coefficient of SES in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate. ß\n\n\n\nCode\nbses &lt;- coef(m1)[\"sei10\"]\n\nci_lower_ses = bses - 1.96 * 0.0023\nci_upper_ses = bses + 1.96 * 0.0023\n\n#transforming confidence intervals of coefficients into odds ratios\nor_ses = exp(bses)\nor_lower_ses = exp(ci_lower_ses)\nor_upper_ses = exp(ci_upper_ses)\n\nbses\n\n\n       sei10 \n-0.006227141 \n\n\nCode\nor_ses\n\n\n    sei10 \n0.9937922 \n\n\nCode\nor_lower_ses\n\n\n    sei10 \n0.9893223 \n\n\nCode\nor_upper_ses\n\n\n    sei10 \n0.9982823 \n\n\nA one unit increase in SES index is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.0062 units (95% CI [-0.0107, -0.0017]), holding all other variables constant. The odds ratio is less than 1 (0.9937922), which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of SES index, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993, or approximately 0.7% per unit increase in SES index, holding other factors constant (95% CI [0.989, 0.998]).\n\n\n\n\nLet’s examine the results on the probability scale.\n\n\nCalculate the marginal effects of sex, age, and SES on mass transportation spending. You can use the margins package function margins discussed in your textbook or you can use the marginaleffects package avg_slope avg_comparisons discussed in lecture. Interpret each estimate.\n\n\n\nCode\n# Calculate the marginal effects using the marginaleffects package\nmarginal_effects &lt;- avg_comparisons(m1, comparison = \"difference\")\n\n# Display the results\nmarginal_effects %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ncontrast\nestimate\nstd.error\nstatistic\np.value\ns.value\nconf.low\nconf.high\n\n\n\n\nage\n+1\n-0.0015153\n0.0005579\n-2.716128\n0.0066050\n7.242216\n-0.0026088\n-0.0004219\n\n\nsei10\n+1\n-0.0015304\n0.0004039\n-3.789361\n0.0001510\n12.692829\n-0.0023219\n-0.0007388\n\n\nsex\nFemale - Male\n0.0630688\n0.0196461\n3.210251\n0.0013262\n9.558495\n0.0245632\n0.1015743\n\n\n\n\n\nCode\n# Extract and interpret the marginal effects\nmarginal_effects_summary &lt;- marginal_effects %&gt;%\n  summarise(\n    term = term,\n    estimate = estimate,\n    std.error = std.error,\n    conf.low = conf.low,\n    conf.high = conf.high,\n    p.value = p.value\n  )\n\n\nmarginal_effects_summary\n\n\n   term     estimate    std.error     conf.low     conf.high      p.value\n1   age -0.001515340 0.0005579046 -0.002608813 -0.0004218672 0.0066050434\n2 sei10 -0.001530384 0.0004038632 -0.002321941 -0.0007388263 0.0001510351\n3   sex  0.063068756 0.0196460526  0.024563200  0.1015743113 0.0013261921\n\n\nCode\nage_estimate &lt;- marginal_effects_summary$estimate[marginal_effects_summary$term == \"age\"]\nage_conf_h &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"age\"]\nage_conf_l &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"age\"]\nage_p &lt;- marginal_effects_summary$p.value[marginal_effects_summary$term == 'age']\n\nses_estimate &lt;- marginal_effects_summary$estimate[marginal_effects_summary$term == \"sei10\"]\nses_conf_h &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sei10\"]\nses_conf_l &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sei10\"]\nses_p &lt;- marginal_effects_summary$p.value[marginal_effects_summary$term == 'sei10']\n\nfem_estimate &lt;- marginal_effects_summary$estimate[marginal_effects_summary$term == \"sex\"]\nfem_conf_h &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sex\"]\nfem_conf_l &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sex\"]\nfem_p &lt;- marginal_effects_summary$p.value[marginal_effects_summary$term == 'sex']\n\n\n\nThe marginal effect of age is -0.0015153 (95% CI [-4.2186724^{-4}, -4.2186724^{-4}]). So, for each additional unit increase of age, the probability of being satisfied with mass transportation spending decreases by approximately -0.151534 percentage points, holding other factors constant (p = 0.006605).\nThe marginal effect of SES is -0.0015304 (95% CI [-7.3882629^{-4}, -7.3882629^{-4}]). For each one-unit increase in the socioeconomic index, the probability of being satisfied with mass transportation spending decreases by approximately -0.1530384 percentage points, holding other variables constant (p = 1.5103506^{-4}).\nThe marginal effect for being female compared to male is 0.0630688 (95% CI [0.1015743, 0.1015743]). This indicates that females are, on average, about 6.3068756 percentage points more likely than males to be satisfied with mass transportation spending, holding other factors constant (p = 0.0013262).\n\n\n\n\n\nNow let’s see whether a person’s political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors.\n\n\nConduct a drop-in-deviance/likelihood ratio test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. Name these two models fit2 and fit3, respectively. Compare the two models.\n\n\n\nCode\nfit2 &lt;- glm(mass_trans_spend_right ~ age + sex + sei10, data = data, family = binomial)\n\nfit3 &lt;- glm(mass_trans_spend_right ~ age + sex + sei10 + polviews, data = data, family = binomial)\n\ntest_likelihoodratio(fit2, fit3) %&gt;% kable()\n\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nfit2\nfit2\nglm\n4\nNA\nNA\nNA\n\n\nfit3\nfit3\nglm\n10\n6\n63.02844\n0\n\n\n\n\n\n\nIs the model with polviews better than the model without?\n\n\nYes.\n\n\n\n\n\nLet’s plot the results\nWe next use the model to produce visualizations:\nGiven the code below, interpret what is being plotted:\n\n-   pol_plot : This plot shows the predicted probability of being satisfied with mass transportation spending across different political views, with error bars representing the 95% confidence intervals.\n\n-   sex_plot : This plot shows the predicted probability of being satisfied with mass transportation spending for males and females, with error bars representing the 95% confidence intervals.\n\n-   ses_plot: This plot shows the predicted probability of being satisfied with mass transportation spending across different levels of socioeconomic status (SES), with a confidence interval band representing the 95% confidence intervals.\n::: callout-tip - adjust the various settings in your plot to make it look professional.\n\nYou can use ggeffects to get the predicted probabilities for these models. :::\n\n\n\nCode\nlibrary(ggeffects)\n\n\ncolors &lt;- c(\"Extremely liberal\" = \"black\",\n            \"Liberal\" = \"#0e2f44\",  # Dark blue\n            \"Slightly liberal\" = \"#1d5a6c\",  # Less dark blue\n            \"Moderate\" = \"#358ca3\",  # Medium blue\n            \"Slghtly conservative\" = \"#71b9d1\",  # Light blue\n            \"Conservative\" = \"#a6dcef\",  # Lighter blue\n            \"Extrmly conservative\" = \"#d0f0fd\")  # Very light blue\n\npp_pol &lt;- ggemmeans(fit3, terms = c(\"polviews\"))\n\n# Adjusted plot with gradient colors\npol_plot &lt;- ggplot(pp_pol, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  scale_color_manual(values = colors) +\n  labs(title = \"Effect of Political Views on Satisfaction with Mass Transportation\",\n       x = \"Political Views\", y = \"Predicted Probability\",\n       color = \"Political Views\") +\n  theme_minimal()\n\npol_plot\n\n\n\n\n\n\n\n\n\nCode\npp_sex &lt;- ggemmeans(fit3, terms = c(\"sex\"))\n\nsex_plot &lt;- ggplot(pp_sex, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  labs(title = \"Effect of Sex on Satisfaction with Mass Transportation\",\n       x = \"Sex\", y = \"Predicted Probability\",\n       color = \"Sex\") +\n  theme_minimal()\n\npp_sex\n\n\n# Predicted probabilities of mass_trans_spend_right\n\nsex    | Predicted |     95% CI\n-------------------------------\nMale   |      0.48 | 0.44, 0.51\nFemale |      0.55 | 0.51, 0.58\n\nAdjusted for:\n*   age = 48.90\n* sei10 = 46.07\n\n\nCode\npp_ses &lt;- ggemmeans(fit3, terms = \"sei10\")\n\n\nses_plot &lt;-  ggplot(pp_ses, aes(x = x, y = predicted)) +\n  geom_line(color = \"#2c7fb8\", size = 1) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#2c7fb8\", alpha = 0.2) +  # Add a confidence interval band\n  labs(title = \"Effect of SES on Satisfaction with Mass Transportation\",\n       x = \"Socioeconomic Status\", y = \"Predicted Probability\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \nses_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs the logistic model a good choice for this data?\n\n\n\nCode\nbinned_residuals(fit2)\n\n\nWarning: About 86% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer: The model is not a very good choice since it doesn’t fit the data well. Only 86% of the residuals are in the erro bounds while &gt;95% are expected.\n\n\n\n\n\n\nCalculate the \\(R^2\\) for this model\n\n\n\nCode\nr2 &lt;- r2_mcfadden(fit2)\n\nR2 = r2$R2\n\n\n\nR2 interpretation: The McFadden’s R2 value for this model is an indicator of the model’s goodness of fit. A higher R2 value suggests a better fit, meaning the model explains a larger proportion of the variance in the response variable. Here only 0.9924817% of the variance are explained suggesting a low fit.\nNext, Take a look at the binned residual plots for each continuous predictor variable and look at linearity. Is there a predictor that sticks out? What can we do to improve model fit in this case?\n\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\")\n\n\nWarning: About 88% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\nCode\nbinned_residuals(fit2, term=\"age\")\n\n\nOk: About 98% of the residuals are inside the error bounds.\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\nCode\nbinned_residuals(fit2, term=\"age\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAge sticks out in a positive way.\n\n\n\n\n\n\n\n\n\n\nCode\nemmeans(fit3, \"polviews\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                   estimate        SE  df z.ratio\n Extremely liberal - Moderate             -0.9266262 0.1950664 Inf  -4.750\n Extremely liberal - Slghtly conservative -0.8487137 0.2127293 Inf  -3.990\n Extremely liberal - Conservative         -0.9935486 0.2108369 Inf  -4.712\n Extremely liberal - Extrmly conservative -1.3402621 0.2792876 Inf  -4.799\n Liberal - Moderate                       -0.7090022 0.1308520 Inf  -5.418\n Liberal - Slghtly conservative           -0.6310897 0.1555805 Inf  -4.056\n Liberal - Conservative                   -0.7759246 0.1532081 Inf  -5.065\n Liberal - Extrmly conservative           -1.1226380 0.2392048 Inf  -4.693\n Slightly liberal - Extrmly conservative  -0.7334002 0.2412625 Inf  -3.040\n p.value\n  &lt;.0001\n  0.0013\n  0.0001\n  &lt;.0001\n  &lt;.0001\n  0.0010\n  &lt;.0001\n  0.0001\n  0.0382\n\nResults are averaged over the levels of: sex \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 7 estimates \n\n\nCode\nemmeans(fit3, \"polviews\", type=\"response\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                 odds.ratio         SE  df null\n Extremely liberal / Moderate              0.3958871 0.07722426 Inf    1\n Extremely liberal / Slghtly conservative  0.4279651 0.09104070 Inf    1\n Extremely liberal / Conservative          0.3702605 0.07806458 Inf    1\n Extremely liberal / Extrmly conservative  0.2617771 0.07311109 Inf    1\n Liberal / Moderate                        0.4921350 0.06439684 Inf    1\n Liberal / Slghtly conservative            0.5320118 0.08277063 Inf    1\n Liberal / Conservative                    0.4602780 0.07051835 Inf    1\n Liberal / Extrmly conservative            0.3254202 0.07784206 Inf    1\n Slightly liberal / Extrmly conservative   0.4802732 0.11587191 Inf    1\n z.ratio p.value\n  -4.750  &lt;.0001\n  -3.990  0.0013\n  -4.712  0.0001\n  -4.799  &lt;.0001\n  -5.418  &lt;.0001\n  -4.056  0.0010\n  -5.065  &lt;.0001\n  -4.693  0.0001\n  -3.040  0.0382\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 7 estimates \nTests are performed on the log odds ratio scale \n\n\n\nConservatives are 2.7027027 times more likely to support mass transit spending compared to extremely liberal and liberal\nExtreme liberals are .37, , .4, .43 times as likely to support spending compared to conservatives, moderates and slight conservatives\nExtreme conservatives are 3.030303, 2.0833333 times more likely to support mass spending than liberals and slight liberals\nLiberals are .49, .46 times more likely to support spending than moderates and slight conservatives.\n\n\n\n\nThe model best predicts satisfaction with mass transportation when it includes age, sex, socioeconomic status (SES), and political views (polviews) as predictors. Specifically, the results indicate that:\n\nConservatives tend to express greater satisfaction with mass transportation compared to liberals.\nFemales report higher satisfaction with mass transportation than males.\nIndividuals with a lower socioeconomic status are more satisfied with mass transportation than those with a higher socioeconomic status.\n\nThese findings suggest that political ideology, gender, and socioeconomic background all play a role in shaping public attitudes toward mass transportation.\n\n\n\n\nDf\nDeviance\nResid. Df\nResid. Dev\nPr(&gt;Chi)\n\n\n\n\nNULL\nNA\nNA\n2589\n3581.340\nNA\n\n\nage\n1\n9.268443\n2588\n3572.072\n0.0023314\n\n\nsex\n1\n12.156624\n2587\n3559.915\n0.0004891\n\n\nsei10\n1\n14.119078\n2586\n3545.796\n0.0001716\n\n\npolviews\n6\n63.028441\n2580\n3482.768\n0.0000000\n\n\n\nTable 1\n\n\n\n\n\nFigure 1: Effect of Sex on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 2: Effect of SES on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 3: Effect of Political Views on Satisfaction with Mass Transportation"
  },
  {
    "objectID": "pages/logistic-regression/index.html#data-general-social-survey",
    "href": "pages/logistic-regression/index.html#data-general-social-survey",
    "title": "Logistic Regression",
    "section": "",
    "text": "The General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "pages/logistic-regression/index.html#eda",
    "href": "pages/logistic-regression/index.html#eda",
    "title": "Logistic Regression",
    "section": "",
    "text": "Let’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then plot the proportion of the response variable, using informative labels for each category.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(modelsummary)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(easystats)\nlibrary(broom)\nlibrary(emmeans)\nlibrary(marginaleffects)\nlibrary(performance)\nlibrary(arm)\nlibrary(modelsummary)\nlibrary(here)\n\n\n\n\nCode\n# Set the working directory\nsetwd(here::here(\"pages/logistic-regression\"))\n# load data\ndata &lt;- read.csv(\"gss2016.csv\")\n# look at the data \nhead(data)\n\n\n      natmass age    sex sei10      region         polviews\n1  Too little  47   Male  87.9 New england         Moderate\n2  Too little  61   Male  38.3 New england          Liberal\n3    Too much  43 Female  21.8 New england         Moderate\n4  Too little  55 Female  39.7 New england Slightly liberal\n5 About right  53 Female  44.6 New england Slightly liberal\n6  Too little  50   Male  80.7 New england Slightly liberal\n\n\nCode\n# See what `natmass` levels exist\nunique(data$natmass)\n\n\n[1] \"Too little\"  \"Too much\"    \"About right\"\n\n\nFill in the “____” below to encode the binary variable\n\n\nCode\ndata &lt;- data %&gt;%\n   mutate(mass_trans_spend_right = as.numeric(natmass == \"About right\"))\nhead(data$mass_trans_spend_right)\n\n\n[1] 0 0 0 0 1 0\n\n\n\n\nCode\n#Get proportions\nmass_spend_summary &lt;- data %&gt;%\n  count(mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if neederd\nmass_spend_long &lt;- mass_spend_summary %&gt;% mutate(opinion=\"Proportion\")\n\n#Factorise for plot\nmass_spend_long$mass_trans_spend_right &lt;- as.factor(mass_spend_long$mass_trans_spend_right)\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\nggplot(mass_spend_long, aes(x = opinion, y = proportion, fill = mass_trans_spend_right)) + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(polviews = factor(polviews,\n                           levels = c(\"Extremely liberal\", \"Liberal\", \"Slightly liberal\",\n                                      \"Moderate\", \"Slghtly conservative\",\"Conservative\", \"Extrmly conservative\"),\n                           ordered = TRUE))\n\n\n\nMake a plot of the distribution of polviews\n\n\n\nCode\n# Get proportions\npoly_summary &lt;- data %&gt;%\n  count(polviews) %&gt;%\n  mutate(proportion_o = n / sum(n))\n\n# Keep the data structure appropriate for plotting\npoly_long &lt;- poly_summary %&gt;% mutate(opinion=\"Proportion\")\n\n# Convert polviews into a factor if not already\npoly_long$polviews &lt;- as.factor(poly_long$polviews)\n\n# Plot the proportions\nggplot(poly_long, aes(x = opinion, y = proportion_o, fill = polviews)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\nWhich political view occurs most frequently in this data set?\nModerate\n\n\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\n\n\n\nCode\n# Plot the relationship between satisfaction with mass transportation spending and political views\nggplot(data, aes(x = polviews, fill = as.factor(mass_trans_spend_right))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Relationship between Political Views and Satisfaction with Mass Transportation Spending\",\n       x = \"Political Views\",\n       y = \"Proportion\",\n       fill = \"Satisfaction with Spending\") +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"green\"), labels = c(\"Not Satisfied\", \"Satisfied\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe more conservative one’s political views are the more they think the amount of spending on mass transportation is correct.\n\nWe’d like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as “89 or older”.\n\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values “89 or older” with a single value.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(age = if_else(age == \"89 or older\", \"89\", age), \n         age = as.numeric(age))\n\n\n\nPlot the frequency distribution of age.\n\n\n\nCode\nggplot(data, aes(x = age)) +\n  geom_histogram(binwidth = 1,color='black', alpha = 0.7) +\n  labs(title = \"Age Distribution\",\n       x = \"Age\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "pages/logistic-regression/index.html#logistic-regression-1",
    "href": "pages/logistic-regression/index.html#logistic-regression-1",
    "title": "Logistic Regression",
    "section": "",
    "text": "Let’s start by fitting a logistic regression model with just the intercept\n\n\n\nCode\nintercept_only_model &lt;- glm(\n  mass_trans_spend_right ~ 1, \n  data = data, \n  family = binomial\n) \n\nintercept_only_model %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.1190594\n0.0393685\n3.024229\n0.0024927\n\n\n\n\n\n\nInterpret the intercept in the context of the data. You can do this by converting the \\(\\beta_0\\) parameter out of the log-odds metric to the probability metric. Make sure to include the 95% confidence intervals. Then interpret the results in a sentence or two–what is the basic thing this probability tells us about?\n\n\n\nCode\nb0 &lt;- coef(intercept_only_model)[1] # get coef\n\nb0_transformed &lt;- exp(b0) / (1 + exp(b0)) # logistic transform\n\nci_lower = b0 - 1.96 * 0.0393685\nci_upper = b0 + 1.96 * 0.0393685\n\n#transforming confidence intervals of coefficients into probabilities\np_lower = exp(ci_lower) / (1 + exp(ci_lower))\np_upper = exp(ci_upper) / (1 + exp(ci_upper))\n\nb0_transformed\n\n\n(Intercept) \n  0.5297297 \n\n\nCode\np_lower\n\n\n(Intercept) \n  0.5104727 \n\n\nCode\np_upper\n\n\n(Intercept) \n  0.5488986 \n\n\nInterpretation: The intercept-only model’s intercept (β0) represents the log-odds of a respondent being satisfied with mass transportation spending when all predictors are at their reference levels. Converting this to the probability metric, the probability of a respondent being satisfied with mass transportation spending is approximately 0.53 (95% CI [0.51, 0.55]). This tells us that, on average, about 53% of respondents are satisfied with the current spending on mass transportation.\n\nNow let’s fit a model using the demographic factors - age,sex, sei10 - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model coefficients (do not display the summary output)\n\n\n\nCode\n#make sure that sex is a factor (i.e. to make sure R knows it's binary/categorical, and not continuous)\ndata &lt;- data %&gt;%\n  mutate(sex = factor(sex, levels = c(\"Male\", \"Female\")))\n\n#fit with glm()\nm1 &lt;- glm(mass_trans_spend_right ~ age + sex + sei10, data = data, family = binomial)\n\n#produce tidy output of model coefficients\nm1 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.5697071\n0.1409061\n4.043169\n0.0000527\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n0.2557439\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\n\nConsider the relationship between sex and one’s opinion about spending on mass transportation. Interpret the coefficient of sex in terms of the logs odds and OR of being satisfied with spending on mass transportation. What are the predicted probabilities for males and females on support for spending on mass transportation? Please include the 95% CIs around each estimate.\n\n\n\nCode\nm1 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.5697071\n0.1409061\n4.043169\n0.0000527\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n0.2557439\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nm1 %&gt;% \n  tidy(exponentiate = TRUE) %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.7677492\n0.1409061\n4.043169\n0.0000527\n\n\nage\n0.9938530\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n1.2914219\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n0.9937922\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nbsex &lt;- coef(m1)[\"sexFemale\"]\n\nci_lower_lo = bsex - 1.96 * 0.0798020\nci_upper_lo = bsex + 1.96 * 0.0798020\n\nci_lower_or = 1.29 - 1.96 * 0.0798020\nci_upper_or = 1.29 + 1.96 * 0.0798020\n\nemm_sex &lt;- emmeans(m1, \"sex\", type = \"response\")\n\nci\n\n\nfunction (x, ...) \n{\n    UseMethod(\"ci\")\n}\n&lt;bytecode: 0x106aabe70&gt;\n&lt;environment: namespace:bayestestR&gt;\n\n\nIf you did this right, you’ll find that being female (as compared to male) is associated with an increase in the log-odds of being satisfied with spending on mass transportation by 0.2557439 units (95% CI [0.09, 0.41]), holding all other variables constant. This equates to the odds of thinking the spending amount is right in females being 1.29 times the odds of thinking this in men (95% CI [1.13, 1.44]).\nThe predicted probability for females to be satisfied with spending on mass transportation is 55.9% (95% CI [53.3%, 58.5%]) and that of males is 49.5% (95% CI [46.7%, 52.4%]).\n\nVerify this.\n\nNext, consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate.\n\n\n\nCode\nbage &lt;- coef(m1)[\"age\"]\n\nci_lower_age = bage - 1.96 * 0.005\nci_upper_age = bage + 1.96 * 0.005\n\n#transforming confidence intervals of coefficients into odds ratios\nor_age = exp(bage)\nor_lower_age = exp(ci_lower_age)\nor_upper_age = exp(ci_upper_age)\n\nbage\n\n\n        age \n-0.00616594 \n\n\nCode\nor_age\n\n\n     age \n0.993853 \n\n\nCode\nor_lower_age\n\n\n      age \n0.9841608 \n\n\nCode\nor_upper_age\n\n\n     age \n1.003641 \n\n\nA one unit increase in age is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by -0.0061659 (95% CI [-0.0159659, 0.0036341]), holding all other variables constant. The odds ratio is 0.993853 (95% CI [0.9841608, 1.0036407])which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of age, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993853, or approximately 0.6146969% per unit increase in age, holding other factors constant.\n\nConsider the relationship between SES and one’s opinion about spending on mass transportation. Interpret the coefficient of SES in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate. ß\n\n\n\nCode\nbses &lt;- coef(m1)[\"sei10\"]\n\nci_lower_ses = bses - 1.96 * 0.0023\nci_upper_ses = bses + 1.96 * 0.0023\n\n#transforming confidence intervals of coefficients into odds ratios\nor_ses = exp(bses)\nor_lower_ses = exp(ci_lower_ses)\nor_upper_ses = exp(ci_upper_ses)\n\nbses\n\n\n       sei10 \n-0.006227141 \n\n\nCode\nor_ses\n\n\n    sei10 \n0.9937922 \n\n\nCode\nor_lower_ses\n\n\n    sei10 \n0.9893223 \n\n\nCode\nor_upper_ses\n\n\n    sei10 \n0.9982823 \n\n\nA one unit increase in SES index is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.0062 units (95% CI [-0.0107, -0.0017]), holding all other variables constant. The odds ratio is less than 1 (0.9937922), which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of SES index, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993, or approximately 0.7% per unit increase in SES index, holding other factors constant (95% CI [0.989, 0.998])."
  },
  {
    "objectID": "pages/logistic-regression/index.html#marginal-effects",
    "href": "pages/logistic-regression/index.html#marginal-effects",
    "title": "Logistic Regression",
    "section": "",
    "text": "Let’s examine the results on the probability scale.\n\n\nCalculate the marginal effects of sex, age, and SES on mass transportation spending. You can use the margins package function margins discussed in your textbook or you can use the marginaleffects package avg_slope avg_comparisons discussed in lecture. Interpret each estimate.\n\n\n\nCode\n# Calculate the marginal effects using the marginaleffects package\nmarginal_effects &lt;- avg_comparisons(m1, comparison = \"difference\")\n\n# Display the results\nmarginal_effects %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ncontrast\nestimate\nstd.error\nstatistic\np.value\ns.value\nconf.low\nconf.high\n\n\n\n\nage\n+1\n-0.0015153\n0.0005579\n-2.716128\n0.0066050\n7.242216\n-0.0026088\n-0.0004219\n\n\nsei10\n+1\n-0.0015304\n0.0004039\n-3.789361\n0.0001510\n12.692829\n-0.0023219\n-0.0007388\n\n\nsex\nFemale - Male\n0.0630688\n0.0196461\n3.210251\n0.0013262\n9.558495\n0.0245632\n0.1015743\n\n\n\n\n\nCode\n# Extract and interpret the marginal effects\nmarginal_effects_summary &lt;- marginal_effects %&gt;%\n  summarise(\n    term = term,\n    estimate = estimate,\n    std.error = std.error,\n    conf.low = conf.low,\n    conf.high = conf.high,\n    p.value = p.value\n  )\n\n\nmarginal_effects_summary\n\n\n   term     estimate    std.error     conf.low     conf.high      p.value\n1   age -0.001515340 0.0005579046 -0.002608813 -0.0004218672 0.0066050434\n2 sei10 -0.001530384 0.0004038632 -0.002321941 -0.0007388263 0.0001510351\n3   sex  0.063068756 0.0196460526  0.024563200  0.1015743113 0.0013261921\n\n\nCode\nage_estimate &lt;- marginal_effects_summary$estimate[marginal_effects_summary$term == \"age\"]\nage_conf_h &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"age\"]\nage_conf_l &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"age\"]\nage_p &lt;- marginal_effects_summary$p.value[marginal_effects_summary$term == 'age']\n\nses_estimate &lt;- marginal_effects_summary$estimate[marginal_effects_summary$term == \"sei10\"]\nses_conf_h &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sei10\"]\nses_conf_l &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sei10\"]\nses_p &lt;- marginal_effects_summary$p.value[marginal_effects_summary$term == 'sei10']\n\nfem_estimate &lt;- marginal_effects_summary$estimate[marginal_effects_summary$term == \"sex\"]\nfem_conf_h &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sex\"]\nfem_conf_l &lt;- marginal_effects_summary$conf.high[marginal_effects_summary$term == \"sex\"]\nfem_p &lt;- marginal_effects_summary$p.value[marginal_effects_summary$term == 'sex']\n\n\n\nThe marginal effect of age is -0.0015153 (95% CI [-4.2186724^{-4}, -4.2186724^{-4}]). So, for each additional unit increase of age, the probability of being satisfied with mass transportation spending decreases by approximately -0.151534 percentage points, holding other factors constant (p = 0.006605).\nThe marginal effect of SES is -0.0015304 (95% CI [-7.3882629^{-4}, -7.3882629^{-4}]). For each one-unit increase in the socioeconomic index, the probability of being satisfied with mass transportation spending decreases by approximately -0.1530384 percentage points, holding other variables constant (p = 1.5103506^{-4}).\nThe marginal effect for being female compared to male is 0.0630688 (95% CI [0.1015743, 0.1015743]). This indicates that females are, on average, about 6.3068756 percentage points more likely than males to be satisfied with mass transportation spending, holding other factors constant (p = 0.0013262)."
  },
  {
    "objectID": "pages/logistic-regression/index.html#model-comparison",
    "href": "pages/logistic-regression/index.html#model-comparison",
    "title": "Logistic Regression",
    "section": "",
    "text": "Now let’s see whether a person’s political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors.\n\n\nConduct a drop-in-deviance/likelihood ratio test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. Name these two models fit2 and fit3, respectively. Compare the two models.\n\n\n\nCode\nfit2 &lt;- glm(mass_trans_spend_right ~ age + sex + sei10, data = data, family = binomial)\n\nfit3 &lt;- glm(mass_trans_spend_right ~ age + sex + sei10 + polviews, data = data, family = binomial)\n\ntest_likelihoodratio(fit2, fit3) %&gt;% kable()\n\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nfit2\nfit2\nglm\n4\nNA\nNA\nNA\n\n\nfit3\nfit3\nglm\n10\n6\n63.02844\n0\n\n\n\n\n\n\nIs the model with polviews better than the model without?\n\n\nYes."
  },
  {
    "objectID": "pages/logistic-regression/index.html#visualization",
    "href": "pages/logistic-regression/index.html#visualization",
    "title": "Logistic Regression",
    "section": "",
    "text": "Let’s plot the results\nWe next use the model to produce visualizations:\nGiven the code below, interpret what is being plotted:\n\n-   pol_plot : This plot shows the predicted probability of being satisfied with mass transportation spending across different political views, with error bars representing the 95% confidence intervals.\n\n-   sex_plot : This plot shows the predicted probability of being satisfied with mass transportation spending for males and females, with error bars representing the 95% confidence intervals.\n\n-   ses_plot: This plot shows the predicted probability of being satisfied with mass transportation spending across different levels of socioeconomic status (SES), with a confidence interval band representing the 95% confidence intervals.\n::: callout-tip - adjust the various settings in your plot to make it look professional.\n\nYou can use ggeffects to get the predicted probabilities for these models. :::\n\n\n\nCode\nlibrary(ggeffects)\n\n\ncolors &lt;- c(\"Extremely liberal\" = \"black\",\n            \"Liberal\" = \"#0e2f44\",  # Dark blue\n            \"Slightly liberal\" = \"#1d5a6c\",  # Less dark blue\n            \"Moderate\" = \"#358ca3\",  # Medium blue\n            \"Slghtly conservative\" = \"#71b9d1\",  # Light blue\n            \"Conservative\" = \"#a6dcef\",  # Lighter blue\n            \"Extrmly conservative\" = \"#d0f0fd\")  # Very light blue\n\npp_pol &lt;- ggemmeans(fit3, terms = c(\"polviews\"))\n\n# Adjusted plot with gradient colors\npol_plot &lt;- ggplot(pp_pol, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  scale_color_manual(values = colors) +\n  labs(title = \"Effect of Political Views on Satisfaction with Mass Transportation\",\n       x = \"Political Views\", y = \"Predicted Probability\",\n       color = \"Political Views\") +\n  theme_minimal()\n\npol_plot\n\n\n\n\n\n\n\n\n\nCode\npp_sex &lt;- ggemmeans(fit3, terms = c(\"sex\"))\n\nsex_plot &lt;- ggplot(pp_sex, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  labs(title = \"Effect of Sex on Satisfaction with Mass Transportation\",\n       x = \"Sex\", y = \"Predicted Probability\",\n       color = \"Sex\") +\n  theme_minimal()\n\npp_sex\n\n\n# Predicted probabilities of mass_trans_spend_right\n\nsex    | Predicted |     95% CI\n-------------------------------\nMale   |      0.48 | 0.44, 0.51\nFemale |      0.55 | 0.51, 0.58\n\nAdjusted for:\n*   age = 48.90\n* sei10 = 46.07\n\n\nCode\npp_ses &lt;- ggemmeans(fit3, terms = \"sei10\")\n\n\nses_plot &lt;-  ggplot(pp_ses, aes(x = x, y = predicted)) +\n  geom_line(color = \"#2c7fb8\", size = 1) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#2c7fb8\", alpha = 0.2) +  # Add a confidence interval band\n  labs(title = \"Effect of SES on Satisfaction with Mass Transportation\",\n       x = \"Socioeconomic Status\", y = \"Predicted Probability\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \nses_plot"
  },
  {
    "objectID": "pages/logistic-regression/index.html#model-assumptions",
    "href": "pages/logistic-regression/index.html#model-assumptions",
    "title": "Logistic Regression",
    "section": "",
    "text": "Is the logistic model a good choice for this data?\n\n\n\nCode\nbinned_residuals(fit2)\n\n\nWarning: About 86% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer: The model is not a very good choice since it doesn’t fit the data well. Only 86% of the residuals are in the erro bounds while &gt;95% are expected."
  },
  {
    "objectID": "pages/logistic-regression/index.html#model-fit",
    "href": "pages/logistic-regression/index.html#model-fit",
    "title": "Logistic Regression",
    "section": "",
    "text": "Calculate the \\(R^2\\) for this model\n\n\n\nCode\nr2 &lt;- r2_mcfadden(fit2)\n\nR2 = r2$R2\n\n\n\nR2 interpretation: The McFadden’s R2 value for this model is an indicator of the model’s goodness of fit. A higher R2 value suggests a better fit, meaning the model explains a larger proportion of the variance in the response variable. Here only 0.9924817% of the variance are explained suggesting a low fit.\nNext, Take a look at the binned residual plots for each continuous predictor variable and look at linearity. Is there a predictor that sticks out? What can we do to improve model fit in this case?\n\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\")\n\n\nWarning: About 88% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\nCode\nbinned_residuals(fit2, term=\"age\")\n\n\nOk: About 98% of the residuals are inside the error bounds.\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\nCode\nbinned_residuals(fit2, term=\"age\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAge sticks out in a positive way."
  },
  {
    "objectID": "pages/logistic-regression/index.html#testing-polviews",
    "href": "pages/logistic-regression/index.html#testing-polviews",
    "title": "Logistic Regression",
    "section": "",
    "text": "Code\nemmeans(fit3, \"polviews\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                   estimate        SE  df z.ratio\n Extremely liberal - Moderate             -0.9266262 0.1950664 Inf  -4.750\n Extremely liberal - Slghtly conservative -0.8487137 0.2127293 Inf  -3.990\n Extremely liberal - Conservative         -0.9935486 0.2108369 Inf  -4.712\n Extremely liberal - Extrmly conservative -1.3402621 0.2792876 Inf  -4.799\n Liberal - Moderate                       -0.7090022 0.1308520 Inf  -5.418\n Liberal - Slghtly conservative           -0.6310897 0.1555805 Inf  -4.056\n Liberal - Conservative                   -0.7759246 0.1532081 Inf  -5.065\n Liberal - Extrmly conservative           -1.1226380 0.2392048 Inf  -4.693\n Slightly liberal - Extrmly conservative  -0.7334002 0.2412625 Inf  -3.040\n p.value\n  &lt;.0001\n  0.0013\n  0.0001\n  &lt;.0001\n  &lt;.0001\n  0.0010\n  &lt;.0001\n  0.0001\n  0.0382\n\nResults are averaged over the levels of: sex \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 7 estimates \n\n\nCode\nemmeans(fit3, \"polviews\", type=\"response\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                 odds.ratio         SE  df null\n Extremely liberal / Moderate              0.3958871 0.07722426 Inf    1\n Extremely liberal / Slghtly conservative  0.4279651 0.09104070 Inf    1\n Extremely liberal / Conservative          0.3702605 0.07806458 Inf    1\n Extremely liberal / Extrmly conservative  0.2617771 0.07311109 Inf    1\n Liberal / Moderate                        0.4921350 0.06439684 Inf    1\n Liberal / Slghtly conservative            0.5320118 0.08277063 Inf    1\n Liberal / Conservative                    0.4602780 0.07051835 Inf    1\n Liberal / Extrmly conservative            0.3254202 0.07784206 Inf    1\n Slightly liberal / Extrmly conservative   0.4802732 0.11587191 Inf    1\n z.ratio p.value\n  -4.750  &lt;.0001\n  -3.990  0.0013\n  -4.712  0.0001\n  -4.799  &lt;.0001\n  -5.418  &lt;.0001\n  -4.056  0.0010\n  -5.065  &lt;.0001\n  -4.693  0.0001\n  -3.040  0.0382\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 7 estimates \nTests are performed on the log odds ratio scale \n\n\n\nConservatives are 2.7027027 times more likely to support mass transit spending compared to extremely liberal and liberal\nExtreme liberals are .37, , .4, .43 times as likely to support spending compared to conservatives, moderates and slight conservatives\nExtreme conservatives are 3.030303, 2.0833333 times more likely to support mass spending than liberals and slight liberals\nLiberals are .49, .46 times more likely to support spending than moderates and slight conservatives."
  },
  {
    "objectID": "pages/logistic-regression/index.html#conclusion",
    "href": "pages/logistic-regression/index.html#conclusion",
    "title": "Logistic Regression",
    "section": "",
    "text": "The model best predicts satisfaction with mass transportation when it includes age, sex, socioeconomic status (SES), and political views (polviews) as predictors. Specifically, the results indicate that:\n\nConservatives tend to express greater satisfaction with mass transportation compared to liberals.\nFemales report higher satisfaction with mass transportation than males.\nIndividuals with a lower socioeconomic status are more satisfied with mass transportation than those with a higher socioeconomic status.\n\nThese findings suggest that political ideology, gender, and socioeconomic background all play a role in shaping public attitudes toward mass transportation.\n\n\n\n\nDf\nDeviance\nResid. Df\nResid. Dev\nPr(&gt;Chi)\n\n\n\n\nNULL\nNA\nNA\n2589\n3581.340\nNA\n\n\nage\n1\n9.268443\n2588\n3572.072\n0.0023314\n\n\nsex\n1\n12.156624\n2587\n3559.915\n0.0004891\n\n\nsei10\n1\n14.119078\n2586\n3545.796\n0.0001716\n\n\npolviews\n6\n63.028441\n2580\n3482.768\n0.0000000\n\n\n\nTable 1\n\n\n\n\n\nFigure 1: Effect of Sex on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 2: Effect of SES on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 3: Effect of Political Views on Satisfaction with Mass Transportation"
  },
  {
    "objectID": "pages/multinomial-regression/index.html",
    "href": "pages/multinomial-regression/index.html",
    "title": "Multinomial Regression",
    "section": "",
    "text": "Lab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight “Why Many Americans Don’t Vote” survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "pages/multinomial-regression/index.html#data",
    "href": "pages/multinomial-regression/index.html#data",
    "title": "Multinomial Regression",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\n\nCode\nlibrary(nnet)\nlibrary(car)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(parameters)\nlibrary(easystats)\n\n\n\n\nCode\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")"
  },
  {
    "objectID": "pages/multinomial-regression/index.html#lrt",
    "href": "pages/multinomial-regression/index.html#lrt",
    "title": "Multinomial Regression",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\n\nCode\n# Run the full model\nmodel &lt;- multinom(voter_category ~ ppage_centered + race + gender + income_cat + educ + pol_ident_new, data = voter_data)\n\n\n# weights:  45 (28 variable)\ninitial  value 6411.501317 \niter  10 value 5818.012349\niter  20 value 5709.034111\niter  30 value 5621.228937\nfinal  value 5616.390878 \nconverged\n\n\nCode\n# Report overall significance of each of the terms\nsummary(model)\n\n\nCall:\nmultinom(formula = voter_category ~ ppage_centered + race + gender + \n    income_cat + educ + pol_ident_new, data = voter_data)\n\nCoefficients:\n         (Intercept) ppage_centered raceHispanic raceOther/Mixed   raceWhite\nsporadic    1.731560     0.04568619   0.04023859      -0.3324253 -0.07753808\nalways      1.490383     0.05820752  -0.34101920      -0.6004751  0.12720746\n          genderMale income_cat$40-75k income_cat$75-125k\nsporadic -0.09005897     -0.0737542388         0.01249916\nalways   -0.19218925     -0.0003681586         0.16519710\n         income_catLess than $40k educHigh school or less educSome college\nsporadic               -0.5878096              -0.8532927       -0.2928893\nalways                 -0.6640936              -1.2672114       -0.3303234\n         pol_ident_newIndep pol_ident_newOther pol_ident_newRep\nsporadic         -0.3924293          -0.940448      -0.08380513\nalways           -0.5623037          -1.400990      -0.16176773\n\nStd. Errors:\n         (Intercept) ppage_centered raceHispanic raceOther/Mixed raceWhite\nsporadic   0.1360829    0.002324481    0.1280060       0.1590604 0.1078495\nalways     0.1468958    0.002565596    0.1497456       0.1853844 0.1187785\n         genderMale income_cat$40-75k income_cat$75-125k\nsporadic 0.07217151         0.1113653          0.1066254\nalways   0.07966046         0.1211677          0.1142763\n         income_catLess than $40k educHigh school or less educSome college\nsporadic                0.1137398              0.09739887       0.09520334\nalways                  0.1270306              0.10882778       0.10240769\n         pol_ident_newIndep pol_ident_newOther pol_ident_newRep\nsporadic         0.09769615          0.1062288        0.1027674\nalways           0.10692855          0.1312004        0.1101431\n\nResidual Deviance: 11232.78 \nAIC: 11288.78 \n\n\nCode\n# Likelihood ratio tests for each term\nlrtest &lt;- car::Anova(model, type = \"II\", test = \"LR\")\nlrtest\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: voter_category\n               LR Chisq Df Pr(&gt;Chisq)    \nppage_centered   638.30  2  &lt; 2.2e-16 ***\nrace              52.65  6  1.379e-09 ***\ngender             6.03  2     0.0491 *  \nincome_cat        67.72  6  1.198e-12 ***\neduc             154.14  4  &lt; 2.2e-16 ***\npol_ident_new    153.84  6  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEverything is significant"
  },
  {
    "objectID": "pages/multinomial-regression/index.html#marginal-effects-political-group---emmeans",
    "href": "pages/multinomial-regression/index.html#marginal-effects-political-group---emmeans",
    "title": "Multinomial Regression",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n\nCode\n# Get estimated marginal means from the model\nmultinomial_analysis &lt;- emmeans(model, ~ pol_ident_new | voter_category)\n\n# Calculate contrasts\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n# You can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\n# Update and display the contrasts\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004"
  },
  {
    "objectID": "pages/multinomial-regression/index.html#marginal-effects-of-education---emmeans",
    "href": "pages/multinomial-regression/index.html#marginal-effects-of-education---emmeans",
    "title": "Multinomial Regression",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n\nCode\n# Get estimated marginal means from the model for education\nmultinomial_analysis_educ &lt;- emmeans(model, ~ educ | voter_category)\n\n# Calculate contrasts for education\ncoefs_educ &lt;- contrast(regrid(multinomial_analysis_educ, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\n\n# Update and display the contrasts for education\nupdate(coefs_educ, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\n\nCode\n  ggemmeans(model, terms = c(\"ppage_centered\")) %&gt;% \n      ggplot(., aes(x = x, y = predicted, fill = response.level)) +\n      geom_area() + \n      geom_rug(sides = \"b\", position = \"jitter\", alpha = .5) + \n      labs(x = \"\\nAge\", y = \"Predicted Probablity\\n\", title = \"Predicted Probabilities of Voting Frequency by Age\") +\n      scale_fill_manual(\n        name = NULL,\n        values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n        labels = c(\"RARELY OR NEVER VOTE    \", \"SOMETIMES VOTE    \", \"ALMOST ALWAYS VOTE    \"),\n        breaks = c(\"rarely/never\", \"sporadic\", \"always\")\n      ) +\n      theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Convert ggemmeans output to a dataframe\npred_data &lt;- ggemmeans(model, terms = \"educ\") %&gt;% as.data.frame()\n\n# Rename columns correctly\ncolnames(pred_data)[colnames(pred_data) == \"x\"] &lt;- \"educ\"  # Rename 'x' to 'educ'\ncolnames(pred_data)[colnames(pred_data) == \"response.level\"] &lt;- \"voter_category\"  # Rename 'response.level' to 'voter_category'\n\n# Ensure factors are properly formatted\npred_data$educ &lt;- factor(pred_data$educ, levels = unique(pred_data$educ))\npred_data$voter_category &lt;- factor(pred_data$voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n\n# Plot the predicted probabilities\nggplot(pred_data, aes(x = educ, y = predicted, fill = voter_category)) +\n  geom_col(position = \"dodge\") +\n  labs(x = \"Education\", y = \"Predicted Probability\", \n       title = \"Predicted Probabilities of Voting Frequency by Education\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nDifferences between political groups and voting behavior - Emmeans\n\n\nCode\nmulti_an &lt;- emmeans(model, ~ pol_ident_new|voter_category)\n\ncoefs = contrast(regrid(multi_an, \"log\"),\"trt.vs.ctrl1\",  by=\"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n\nCode\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\n\nCode\nmulti_an &lt;- emmeans(model, ~ educ|voter_category)\n\ncoefs = contrast(regrid(multi_an, \"log\"),\"trt.vs.ctrl1\",  by=\"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\nCode\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.799\n0.095\n28\n-8.416\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.278\n0.092\n28\n-3.030\n0.014\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.520\n0.088\n28\n5.920\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.188\n0.104\n28\n-11.394\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.310\n0.097\n28\n-3.207\n0.009\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.878\n0.098\n28\n8.995\n0.000\n\n\n\n\n\n\n\nInterpretation of the Marginal Effects Analysis\nThe table presents pairwise comparisons of predicted probabilities for voter categories based on education levels. The contrasts indicate how education level affects the likelihood of being in different voter categories (sporadic or always voters) compared to rarely/never voters.\n\nSporadic Voting vs. Rarely/Never Voting\n\nHigh School or Less vs. College: Estimate = -0.799 (p &lt; .001)\n→ Those with a high school education are significantly less likely to vote sporadically compared to college graduates.\n\nSome College vs. College: Estimate = -0.278 (p = .014)\n→ Those with some college education are also less likely to vote sporadically than college graduates, but the effect is smaller.\nSome College vs. High School or Less: Estimate = 0.520 (p &lt; .001)\n→ Those with some college education are more likely to vote sporadically compared to high school graduates.\n\nAlways Voting vs. Rarely/Never Voting\n\nHigh School or Less vs. College: Estimate = -1.188 (p &lt; .001)\n→ High school graduates are far less likely to always vote compared to college graduates.\nSome College vs. College: Estimate = -0.310 (p = .009)\n→ Those with some college education are also less likely to always vote than college graduates, but the effect is smaller.\nSome College vs. High School or Less: Estimate = 0.878 (p &lt; .001)\n→ Those with some college education are significantly more likely to always vote compared to high school graduates.\n\n\n\n\nConclusion\n\nHigher education is associated with more frequent voting behavior.\nCollege graduates are the most consistent voters (higher probability of being in the “Always” category).\nPeople with only a high school diploma are the least likely to vote regularly.\nSome college education increases voting probability compared to only high school but does not reach the level of full college graduates."
  }
]